{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Model for California Real Estate Values\n",
    "\n",
    "    Adapted from Data Science book used in Practical Data Analysis class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location to save the pickled trained model\n",
    "\n",
    "\n",
    "You need to provide the full path to the pickled file, or else TabPy will fail to load it. \n",
    "Change the C:\\\\Code\\\\Python path as you wish\n",
    "#joblib.dump(final_model2, \"C:\\\\Code\\\\Python\\\\CA_real_estate_model2.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data\n",
    "   Template code to download compressed file, uncompress and save to local system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # used just to create the folders on current directory\n",
    "import tarfile\n",
    "from six.moves import urllib # Note needed for Python3 -- compatibility with Python2\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "fetch_housing_data() # end result is housing.csv file imported to Jupyter local directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the version of Scikit-learn\n",
    "\n",
    "#import nltk\n",
    "import sklearn\n",
    "\n",
    "#print('The nltk version is {}.'.format(nltk.__version__))\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The scikit-learn version used for this demo is 0.22.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load df \"housing\"\n",
    "Contains the original data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals for this exercise\n",
    "    1) Predict the value of median_house_value attribute (target label)\n",
    "    2) We know median_income is a strong predictor of median_household_value. We will create the training set stratified by median_income\n",
    "    3) Input attributes total_rooms, total_bedrooms and population are SUM by district and therefore useless to draw any correlations. We know this is needed by visually inspecting the source data.\n",
    "    We will derive average values by dividing those attributes by the number of households\n",
    "    4) Categorical attribute ocean_proximity will be changed to numeric using OneHotEncoding\n",
    "    5) Missing values will be replaced with the average for the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratify the split by Median Income range\n",
    "    Because Median Income is an important indicator of house values\n",
    "    We want to ensure all median income ranges are represented in the training set\n",
    "    We don't have a range indicator in the dataset, so we will create a function to classify the median income into ranges, then run the split. We will drop the range indicator when we are done with the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a category for median income ranges. Limit it to 5.\n",
    "\n",
    "# Divide by 1.5 to limit the number of income categories\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "da=housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into training and test sets\n",
    "    Use sklearn to perform stratified split by Median Income ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need \"income_cat\" anymore. Drop it from split sets\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the test set to CSV, so we cn reuse it in Tableau \n",
    "strat_test_set.to_csv(\"c:\\Code\\Python\\housing_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training set to CSV, so we cn reuse it in Tableau \n",
    "strat_train_set.to_csv(\"c:\\Code\\Python\\housing_training.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training set for ML algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Split input attributes and target(label)\n",
    "    Drop median_house_value from the training set\n",
    "    \n",
    "    Data frame \"housing\" is now of INPUT ATTRIBUTES only! \n",
    "    Data frame housing_num contains on numeric attributes --> only used to grab numeric column names -- is there a smarter way?\n",
    "    Array housing_labels is the saved copy of target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels from training set\n",
    "housing_num = housing.drop('ocean_proximity', axis=1) # only the numeric values\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy() # save labels from training set for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Define sklearn transformation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a custom function to create derived columns rooms_per_household and population_per_household\n",
    "Nulls and categories will be handled with standard sklearn transformation classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# get the right column indices: safer than hard-coding indices 3, 4, 5, 6\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = [\n",
    "    list(housing.columns).index(col)\n",
    "    for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\")]\n",
    "\n",
    "#add_extra_features is your custom function\n",
    "def add_extra_features(X, add_bedrooms_per_room=True):\n",
    "    rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "    population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "    if add_bedrooms_per_room:\n",
    "        bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "        return np.c_[X, rooms_per_household, population_per_household,\n",
    "                     bedrooms_per_room]  # concatenate new columns to existing columns in X\n",
    "    else:\n",
    "        return np.c_[X, rooms_per_household, population_per_household]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Apply all 4 transformations in a Pipeline\n",
    "    imputer = Handle nulls - replace all nulls values with the average value of the column\n",
    "    std_scaler = apply default feature scaler to all numbers\n",
    "    attribs_adder = custom function that creates derived columns rooms, bedrooms and population per household\n",
    "    OneHotEncoder = changes category \"ocean_proximity\" to numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler # to scale numbers\n",
    "from sklearn.impute import SimpleImputer # to handle nulls\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_attribs = list(housing_num)   # List of numeric columns\n",
    "cat_attribs = [\"ocean_proximity\"] # List of categorical attributes\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),   # apply numeric pipeline to numeric attributes\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),# apply category pipeline to categories\n",
    "    ])\n",
    "\n",
    "# housing is the training data without the labels\n",
    "# It's not really necessary to separate categorical from numerical attributes, the ColumnTransformer applies\n",
    "#    functions to the selected numeric or category columns\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing_prepared = pd.DataFrame(\n",
    "    housing_prepared,\n",
    "    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\", \"cat1\", \"cat2\", \"cat3\", \"cat4\", \"cat5\"])\n",
    "# Check that all columns look good with their labels\n",
    "df_housing_prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select and train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try the full preprocessing pipeline on a few training instances\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "# Compare predictions to actual values\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate how good the model is \n",
    "    1) How does the model fit my training data? \n",
    "    2) How does the model fit another dataset - not the training data\n",
    "    \n",
    "    If the model does not fir the training data, then the model is not good - for example, this data does not fit a linear function, so using linear regression will not produce good predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate mean_squared_error and mean_absolute_error\n",
    "    In a Linear regression, our objective is to minimize the MSE. This translates to calculating points on the line of best fit that are closest to the actual points on the graph\n",
    "    \n",
    "    We will calculate some statistical error mesasures comparing the predicted values to the known labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meaning of RMSE\n",
    "As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance,\n",
    "and has the useful property of being in the ***same units as the response variable. \n",
    "\n",
    "Lower values of RMSE indicate better fit.\n",
    "\n",
    "RMSE is a good measure of how accurately the model predicts the response, and it is the most important criterion \n",
    "for fit if the main purpose of the model is prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)  # run prediction on entire training dataset\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lin_mae = mean_absolute_error(housing_labels, housing_predictions)\n",
    "lin_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay,  this  is  better  than  nothing  but  clearly  not  a  great  score:  most  districts’\n",
    "median_housing_values  range between $120,000 and $265,000, so a typical prediction error of $68,628 is not very satisfying.\n",
    "(See 25th and 75th percentiles below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coeficient and Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_housing_prepared.columns)\n",
    "print ('Coeficients: \\n,', lin_reg.coef_)\n",
    "print ('Intercept: \\n', lin_reg.intercept_)\n",
    "\n",
    "for i in range(len(df_housing_prepared.columns)):\n",
    "    print ('Coefficient of ', df_housing_prepared.columns[i], ' = ', lin_reg.coef_[i])\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion - error too large - UNDERFITTING\n",
    "    The Linear Regression model is off by $49k (MAE) or $68k (MSE)\n",
    "    You model does not even fir the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as  plt\n",
    "plt.scatter(df_housing_prepared['population_per_household'], housing_labels, color='red')\n",
    "plt.plot(df_housing_prepared['population_per_household'], housing_predictions, color='blue')\n",
    "plt.title='Population per Houselhold  vs House Price'\n",
    "plt.xlabel='Population per Household'\n",
    "plt.ylabel='House Price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model: Decision Tree Regressor\n",
    "A decision tree is a supervised machine learning model used to predict a target by learning decision rules from features. As the name suggests, we can think of this model as breaking down our data by making a decision based on asking a series of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model metric: RMSE (Root Mean Squared Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE = 0 Model not good either = OVERFITTING\n",
    "    Means that every predicted value is exaclty the same as the real value. \n",
    "    You model fits the training dataset TOO WELL ==> it cannot generlize and make predictions on values it never saw before\n",
    "    \n",
    "    If the model does not capture the dominant trend that we can all see (positively increasing, in our case), it can’t predict a likely output for an input that it has never seen before — defying the purpose of Machine Learning to begin with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how model did not make any generalizations. All predicted points are on top of target values\n",
    "import matplotlib.pyplot as  plt\n",
    "plt.scatter(df_housing_prepared['population_per_household'], housing_labels, color='red')\n",
    "plt.plot(df_housing_prepared['population_per_household'], housing_predictions, color='blue')\n",
    "plt.title='Population per Houselhold  vs House Price'\n",
    "plt.xlabel='Population per Household'\n",
    "plt.ylabel='House Price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "max_depth refers to the maximum depth of the tree and n_estimators, the number of trees in the forest. Ideally, you can expect a better performance from your model when there are more trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "median_housing_values  range between $120,000 and $265,000\n",
    "Error of 22k (about 18%) is acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example using GridSearch to tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Perform Grid-Search\n",
    "gsc = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(),\n",
    "    param_grid={\n",
    "        'max_depth': range(3,7),\n",
    "        'n_estimators': (10, 50, 100, 1000),\n",
    "        },\n",
    "    cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "grid_result = gsc.fit(housing_prepared, housing_labels)\n",
    "best_params = grid_result.best_params_\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],random_state=False, verbose=False)\n",
    "# Perform K-Fold CV\n",
    "scores = cross_val_score(rfr, housing_prepared, housing_labels, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest2_reg = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],random_state=False, verbose=False)\n",
    "forest2_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = forest2_reg.predict(housing_prepared)\n",
    "forest2_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest2_rmse = np.sqrt(forest2_mse)\n",
    "forest2_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running RandomForest with 100 estimators made the fit to the data set worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold Cross Validation\n",
    "    Cross-validation is a statistical method used to estimate the skill of machine learning models.Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n",
    "    \n",
    "    Three common tactics for choosing a value for k are as follows:\n",
    "\n",
    "Representative: The value for k is chosen such that each train/test group of data samples is large enough to be statistically representative of the broader dataset.\n",
    "\n",
    "k=10: The value for k is fixed to 10, a value that has been found through experimentation to generally result in a model skill estimate with low bias a modest variance.\n",
    "\n",
    "k=n: The value for k is fixed to n, where n is the size of the dataset to give each test sample an opportunity to be used in the hold out dataset. This approach is called leave-one-out cross-validation.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-tree_scores)\n",
    "print ('MSE scores')\n",
    "display_scores(tree_scores)\n",
    "print('')\n",
    "print ('RMSE scores')\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We already know the tree and liner models don't fit the training data. We applied the cross val for education purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Model fit to the training set')\n",
    "print ('-----------------------------')\n",
    "print ('Linear regression RMSE = ', lin_rmse)\n",
    "print ('Decision Tree RMSE = ', tree_rmse)\n",
    "print ('Random Forest RMSE = ', forest_rmse)\n",
    "\n",
    "\n",
    "print ('')\n",
    "print ('Model fit based on 10-Fold Validation')\n",
    "print ('-------------------------------------')\n",
    "print ('Linear Regression Mean RMSE Scores = ', lin_rmse_scores.mean(), ' ~ STD: ', lin_rmse_scores.std())\n",
    "print ('Decision Tree RMSE Scores = ', tree_rmse_scores.mean(), ' ~ STD: ', tree_rmse_scores.std())\n",
    "print ('Random Forest RMSE Scores = ', forest_rmse_scores.mean(), ' ~ STD: ', forest_rmse_scores.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune your models\n",
    "    Look for the best combination of hyperparameters for your selected model (RandomForest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "   ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels) # Best model is stored in grid_search variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameter combination found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Randomized Search\n",
    "    Use this when you don't know the specific values you want to try. This uses a large range with min/max values for estimators and features\n",
    "    n_iter=10 = will try 10 combinations of randomly selected parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200),\n",
    "        'max_features': randint(low=1, high=8),\n",
    "    }\n",
    "# n_iter=10 = will try 10 combinations of randomly selected parameters\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
    "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "rnd_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply to test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model\n",
    "    We need the saved TESTING set we split out of the original dataset earlier: strat_test_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model1 = grid_search.best_estimator_\n",
    "final_model2 = rnd_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions1 = final_model1.predict(X_test_prepared)\n",
    "final_predictions2 = final_model2.predict(X_test_prepared)\n",
    "\n",
    "final_mse1 = mean_squared_error(y_test, final_predictions1)\n",
    "final_rmse1 = np.sqrt(final_mse1)\n",
    "print(final_rmse1)\n",
    "\n",
    "final_mse2 = mean_squared_error(y_test, final_predictions2)\n",
    "final_rmse2 = np.sqrt(final_mse2)\n",
    "print(final_rmse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# Change the path to the pickled file to your preferred location\n",
    "# If a path is not provided, Jupyter saves the file to the same folder Jupyter was launched from (in my case, C:\\Code\\Python)\n",
    "# Witout a full path, the TabPy server will not be able to locate and load the picked model! \n",
    "\n",
    "joblib.dump(final_model2, \"C:\\\\Code\\\\Python\\\\CA_real_estate_model2.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TabPy endpoint for selected model\n",
    "    The input for the model must match the format of the training dataset used to create and train the model. \n",
    "    In the model creation, several transformations were applied to the data, and that code must be applied in '\n",
    "    the TabPy endpoint as well. \n",
    "    \n",
    "    In Tableau, the workbook will connect to an Excel file containing data from the test dataset created in this Jupyter notebook, in exactly the same layout. Tableau will then call the PredctHomeValue function via the \"RealEstateDemo\" endpoint to obtain the predicted real estate values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PredictHomeValue(longitude,latitude,housing_median_age,total_rooms,total_bedrooms,\n",
    "                     population,households,median_income,ocean_proximity):\n",
    "    # Load arguments to pandas dataframe\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    housing = pd.DataFrame({\n",
    "     \"longitude\": longitude, \"latitude\": latitude, \"housing_median_age\": housing_median_age,\"total_rooms\": total_rooms,\n",
    "     \"total_bedrooms\": total_bedrooms,\"population\": population,\"households\": households,\"median_income\": median_income,\n",
    "      \"ocean_proximity\": ocean_proximity\n",
    "    })\n",
    "    \n",
    "   # Run the test data through pipeline to cleanse the datataset \n",
    "\n",
    "    from sklearn.preprocessing import FunctionTransformer\n",
    "    housing_num = housing.drop('ocean_proximity', axis=1) # only the numeric values\n",
    "\n",
    "    # get the right column indices: safer than hard-coding indices 3, 4, 5, 6\n",
    "    rooms_ix, bedrooms_ix, population_ix, household_ix = [\n",
    "        list(housing.columns).index(col)\n",
    "        for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\")] \n",
    " \n",
    "    #add_extra_features is your custom function\n",
    "    def add_extra_features(X, add_bedrooms_per_room=True):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                     bedrooms_per_room]  # concatenate new columns to existing columns in X\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "    \n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler # to scale numbers\n",
    "    from sklearn.impute import SimpleImputer # to handle nulls\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    num_attribs = list(housing_num)   # List of numeric columns\n",
    "    cat_attribs = [\"ocean_proximity\"] # List of categorical attributes\n",
    "\n",
    "    num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "\n",
    "    full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),   # apply numeric pipeline to numeric attributes\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),# apply category pipeline to categories\n",
    "        ])\n",
    "\n",
    "    housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "    # Load previously saved model and use it for predictions\n",
    "    # Use the full path where picked file is saved. Jupyter finds the model on the same folder it was saved\n",
    "    # TabPy needs the full reference to locate the file \n",
    "    \n",
    "    from sklearn.externals import joblib\n",
    "    my_model_loaded = joblib.load(\"C:\\\\Code\\\\Python\\\\CA_real_estate_model2.pkl\") \n",
    "\n",
    "    final_predictions = my_model_loaded.predict(housing_prepared)\n",
    "\n",
    "    return list(final_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[446339.27777777775,\n",
       " 385031.9666666667,\n",
       " 308553.4611111111,\n",
       " 204964.49444444446,\n",
       " 166965.58333333334,\n",
       " 180360.5888888889,\n",
       " 279915.1722222222,\n",
       " 391298.52777777775,\n",
       " 108102.78888888888]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredictHomeValue(_arg1,_arg2,_arg3,_arg4,_arg5,_arg6,_arg7,_arg8,_arg9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpy.tabpy_tools.client import Client\n",
    "\n",
    "connection = Client('http://localhost:9004/')\n",
    "# Publish the  function to TabPy server \n",
    "connection.deploy('RealEstateDemo',\n",
    "                  PredictHomeValue,\n",
    "                  'Prediction Model for Property Values',\n",
    "                 override = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of jupyter notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
